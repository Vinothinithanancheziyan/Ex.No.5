# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
## Name: VINOTHINI T
## Reg No: 212223040245

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

## **AI Tool Used**

* Google Gemini

---

## **Definition of Prompt Types**

* **Naïve Prompt**: Simple, vague, or open-ended request without proper guidance.
* **Structured Prompt**: Detailed, contextual, and precise instruction that narrows down the expected response.

---

## **Methodology**

1. Identify multiple test scenarios: story generation, factual Q\&A, summarization, and advice.
2. Prepare two prompt versions for each task (naïve vs structured).
3. Run the experiment with ChatGPT and capture both outputs.
4. Evaluate outputs on:

   * **Quality** → Clarity & structure of the response
   * **Accuracy** → Correctness of information
   * **Depth** → Completeness & reasoning

---

## **Observations**

| **Scenario**   | **Naïve Prompt & Response**                                       | **Structured Prompt & Response**                                                                                                           | **Evaluation**                                   |
| -------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| Story Writing  | Prompt: “Write a story.” → Output: Very short, general story.     | Prompt: “Write a 150-word inspirational story about teamwork.” → Output: Focused, detailed, with a clear moral.                            | Structured prompt gave higher quality and depth. |
| Factual Answer | Prompt: “Explain about sun.” → Output: Few general facts.         | Prompt: “Explain the sun’s structure, energy source, and role in the solar system.” → Output: Detailed with scientific terms and accuracy. | Structured prompt improved accuracy.             |
| Summarization  | Prompt: “Summarize pollution.” → Output: Just 1–2 lines, unclear. | Prompt: “Summarize the causes, effects, and solutions of air pollution in under 120 words.” → Output: Organized, clear, and concise.       | Structured prompt gave clarity and depth.        |
| Advice         | Prompt: “Give me some tips.” → Output: Generic advice.            | Prompt: “Suggest three time-management tips for college students during exams.” → Output: Specific, actionable, and relevant.              | Structured prompt generated practical insights.  |

---

## **Analysis**

* Naïve prompts → produce vague, short, and unfocused responses.
* Structured prompts → deliver clear, detailed, and more accurate information.
* Across all scenarios, structured prompts consistently outperformed naïve prompts.
* Only for very **simple queries**, naïve prompts were acceptable.

---

<img width="518" height="440" alt="image" src="https://github.com/user-attachments/assets/bac89b2d-2ce9-41ab-9f8c-e610c250349e" />

## **Conclusion**

This experiment shows that **prompt clarity directly impacts AI output quality**. By providing structured instructions, ChatGPT responses become **richer, more accurate, and task-oriented**, making structured prompting the preferred approach for practical applications.

---

## **Result**

The comparative analysis of naïve and structured prompts was successfully carried out and the expected results were obtained.

---
